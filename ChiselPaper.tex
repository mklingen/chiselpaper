%\documentclass{article} % For LaTeX2e
\documentclass[conference,10pt]{IEEEtran}
%\usepackage{nips14submit_e,times}

\usepackage[sc]{mathpazo}
%\linespread{1.05}         % Palatino needs more leading (space between lines)
\usepackage[T1]{fontenc}

\usepackage{url}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{color}
\definecolor{orange}{rgb}{0.8,0.4,0}
\definecolor{mylink}{RGB}{18,68,115}
\definecolor{darkgreen}{rgb}{0.3,0.6,0.3}

\usepackage{graphicx}
%\usepackage[footnotesize]{caption}
\usepackage[numbers,sort&compress]{natbib}

\hypersetup{letterpaper,bookmarksopen,bookmarksnumbered,
pdfpagemode=UseOutlines,
colorlinks=true,
linkcolor=mylink,
anchorcolor=blue,
citecolor=mylink,
filecolor=blue,
menucolor=blue,
urlcolor=mylink,
}
%\usepackage{multicol}
 
% Labels in IEEE format
% Equation
\newcommand{\eref}[1]{Eq.(\ref{#1})}
% Figure
\newcommand{\figref}[1]{Fig.\ref{#1}}

\usepackage{ifthen}

\newboolean{include-notes}
\setboolean{include-notes}{true}
% http://en.wikibooks.org/wiki/LaTeX/Colors
\newcommand{\ssnote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{orange}{\textbf{SS: #1}}}{}}
\newcommand{\adnote}[1]{\ifthenelse{\boolean{include-notes}}%
 {\textcolor{LimeGreen}{\textbf{AD: #1}}}{}}

\usepackage[font=footnotesize]{caption}
\usepackage{subfigure}

\title{\Large Real-time Surface Reconstruction on a Mobile Device \\
using Chunked Truncated Signed Distance Fields}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version
\usepackage{amsmath,amssymb,amsthm}
\begin{document}

\maketitle

\begin{abstract}
Abstract here
\end{abstract}

\section{Introduction}
\begin{itemize}
    \item We want to do large-scale 3D reconstruction from depth data with only
    limited hardware.
    \item This is attractive for many use cases where all one wants to do is
    make a rough mesh of an area. The real-time aspect allows the user to see
    the accuracy of the reconstruction easily. This is also attractive for small
    mobile robots (such as quad-copters) with limited onboard compute power.
    \item We have access to VIO, and so don't need to do pose estimation
    \item Existing techniques are not sufficient for the amount of compute power
    we have
    \item We want to be careful about maintaining probabalistically accurate
    data and not throwing it away/compressing it.
    \item It's important to keep track of the noise model of the sensor to deal
    with noise, distortion and missing data.
    \item We want to use the TSDF over occupancy grids \cite{Elfes1989} or
    octomaps \cite{Wurm2010}, because the TSDF is much better at maintaining
    surface information, and occupancy information can easily be extracted from
    it.
    \item We also want to use the color camera, but don't want to make a
    photometrically accurate model due to our limited compute power.
\end{itemize}

\section{Related Work}
\begin{itemize}
    \item Real-time mapping solutions started with occupancy grids
    \cite{Elfes1989}. But implemented in a naive way, these are too memory
    intensive for our purposes. They are also bad at surface reconstruction.
    \item More recent work in occupancy grid mapping reduces their memory
    footprint by introducing an octree structure \cite{Wurm2010}. However,
    octree structures have a hefty lookup time for each access, read, or write.
    They also have poor cache performance during iteration.
    \item \cite{Curless1996} introduced the TSDF as an efficient, accurate way
    of reconstructing surfaces from many registered depth images. These are more
    desirable for surface reconstruction than occupancy grids, because they
    maintain local structure (inside vs. outside).
    \item \cite{Newcombe}, introduced a method (Kinect Fusion) of creating and
    registering TSDF volumes to depth images in real-time using ICP-like alignment of scans.
    Making heavy use of GPU computing, Kinect fusion is able to generate and
    display extremely high-quality reconstructions in a small area (due to
    using a fixed size volume for reconstruction). 
    \item Attempting to increase the size of Kinect Fusion reconstructions and
    reduce drift, Kintinuous \cite{Whelan2013} keeps a running window of the
    scene that gets integrated into a TSDF. Parts of the scene outside the
    window are turned into a mesh with simple greedy triangulation. While this
    allows them to make larger reconstructions than Kinect Fusion, they lose
    distance field data as they move around the scene -- data which might still
    be useful in its uncompressed form. We want to do large-scale reconstruction
    \emph{without} compressing volumetric data into a surface representation
    first.
    \item Like \cite{Whelan2013}, \cite{Bylow2013} use the volume structure
    directly to store colors. We copy the method of \cite{Bylow2013} to colorize
    our meshes due to the simplicity and speed of the method.
\end{itemize}

\section{Approach}
\begin{itemize}
    \item We start by getting an accurate probablistic model of the depth error
    on the sensor by training a quadratic model of the depth and noise per
    pixel. Depth images are compensated by this model before being passed into
    our system.
    \item We get pose by monocular visual-intertial odometry, rather than by
    directly estimating pose from the depth image.
    \item Depth comes in at 3-5Hz, as does RGB, but never at the same time.
    \item Explain the TSDF
    \item We represent the world as a collection of fixed size 16x16x16
    \emph{Meta-Voxels} or \emph{Chunks}, aligned next to each other. Chunks are
    allocated only when rays from the sensor would cause them to be updated.
    They are garbage collected whenever there is no voxel with no weight in it.
    \item To update a chunk with depth data, we can do one of two things:
    \emph{Raycasting}, or \emph{Voxel Projection}. In the raycasting case, we
    efficiently march rays through voxels in each updated TSDF volume, and
    update the signed distance and weight there. In the voxel projection case,
    we project the center of each voxel onto the image, and according to the
    distance from the center of the voxel as compared with the depth from the
    sensor, we either add weight to it, carve it, or do nothing to it.
    \item Raycasting is O(number of rays), while Voxel Projection is O(number
    of voxels). Voxel projection is an aliased approximation of raycasting. 
    \item To do color, we project the points along the ray we are updating onto
    the color image. We then update a separate \emph{Color Chunk} with data from
    the RGB image.
    \item We use a dynamic trunction distance based on the sensor model.
    \item Rendering is handled by incrementally creating meshes of the updated
    chunks using Marching Cubes. Chunks are frustum culled based on the frustum
    of the renderer's camera, and only those which are near the camera and
    inside its frustum are rendered. Faraway chunks are rendered as boxes. This
    allows us to very efficiently render huge areas using only the fixed
    function pipeline -- an important feature for devices with limited graphics
    capability.
\end{itemize}

\section{Results}
\begin{itemize}
    \item We are able to reconstruct very large areas at a relatively high (3
    cm) resolution on a mobile device.
    \item We have a limited memory footprint, and each frame is processed on
    the CPU in less than 150 ms.
    \item Show comparisons of different parts of our technique being used
    (voxel carving vs. not, projection vs. raycasting) in terms of speed, memory
    efficiency, and so on. Also show pictures.
    \item Show the difference between merely doing online odometry vs. offline
    bundle adjustment
\end{itemize}

\section{Discussion and Future Work}
\begin{itemize}
    \item If we're keeping around the volumetric information, what does that buy
    us? Can it help us with loop closure?
    \item These techniques can be used outside of the mobile context to speed
    things up.
    \item We gain a lot by using decoupled pose, but there needs to be a way to
    incorporate the VIO pose with dense alignment (like they do in Kintinuous).
\end{itemize}

\section{Acknowledgements}
\begin{itemize}
    \item Thank project Tango, ATAP, the team.
\end{itemize}

\bibliographystyle{plain}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{densemapping, ./densemapping} 
\end{document}